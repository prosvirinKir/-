\chapter*{Заключение}
\addcontentsline{toc}{chapter}{Заключение}

В~настоящей выпускной квалификационной работе автором были предложены несколько принципиально различных архитектурных решений
предиктивных моделей машинного обучения на~основе нейронных сетей:
\begin{itemize}[noitemsep, topsep=-.5\parskip, leftmargin=!]
	\item полносвязная нейронная сеть;
	\item рекуррентные нейронные сети (RNN и GRU);
	\item автокодировщик.
\end{itemize}

\vspace{.5\parskip}
Каждая из~этих моделей была призвана прогнозировать опасный отказ верхних строений железнодорожного пути.\linebreak
Принимая на~вход данные о~состоянии объектов железнодорожного пути, модель возвращает вероятность отказа каждого из~участков.

Полносвязная нейронная сеть~--- простейший вариант архитектуры~--- был рассмотрен автором в~первую очередь.
Данный тип сетей является довольно распространенным и используется для~решения широкого круга задач, в~том числе задачи классификации объектов.

Логичным продолжением исследования стало рассмотрение применимости рекуррентных нейронных сетей, 
которые исходно разрабатывались именно для~работы с~последовательностями (временными рядами).
Эксперименты проведены с~сетью на~основе нейронной сети~Элмана (RNN)
и~сетью на~основе управляемых рекуррентных блоков (GRU).

Также был рассмотрен такой тип нейронных сетей как автокодировщик.
Это архитектура, позволяющая применять обучение без учителя
при использовании метода обратного распространения ошибки.
Идея применения их в~приведенной задаче состоит в~том, чтобы перевести
все объекты в~некоторое латентное пространство меньшей размерности и~применить
метод ближайших соседей для их классификации.

Лучше всего с поставленной задачей справилась рекуррентная нейронная сеть на основе управляемых рекуррентных блоков (GRU).
Полученный результат превосходит остальные модели на~основе нейронных сетей, причем как по~итоговому качеству, так и~по~скорости работы.
Кроме того, результаты работы рекуррентных нейронных сетей превосходят результаты,
полученные на основе различных вариантов градиентного бустинга.

В~дальнейшем можно постараться улучшить результаты работы,
рассмотрев нейронные сети с~архитектурой LSTM (Long Short-Term Memory),
на~основе внимания (Attention Neural Network) и Transformers.
При этом следует учесть, что порог по точности
с учетом примерно 17\,\% шума в объектах положительного класса
практически достигнут.
Незначительное увеличение точности, скорее всего, возможно получить
за счет существенного увеличения времени обучения
и дополнительных сложностей в применении моделей на практике.
